# Lesson: Working with the Titanic Dataset using TensorFlow

## Introduction to the Titanic Dataset

For our section on working with tabular data, we'll use the Titanic as our dataset. The RMS Titanic was a British passenger liner that sank in the Atlantic Ocean in 1912 after hitting an iceberg. The Titanic was on her maiden voyage from Southampton, England, to New York City, stopping off at Cherbourg, France, and Queenstown, Ireland.

One of the benefits of using TensorFlow is its built-in APIs that support input pipelines, which are great for working with tabular data. With the `tf.data` API, we can handle large amounts of data, build complex input pipelines, read from different data formats, and perform complex transformations.

### Key Concepts

- **Dataset Abstraction**: Represents a sequence of elements, where each element is a single training example or row.
- **Components**: Each element consists of tensor components representing columns related to a passenger and a label indicating survival (0 for did not survive, 1 for survived).

### Creating an Input Pipeline

To create an input pipeline, we need a data source. For example, to construct a dataset from data in memory, you can use `from_tensors` or `from_tensor_slices`.

## Comparing the Titanic Model to the Fashion-MNIST Model

To understand the Titanic dataset, let's compare it with the Fashion-MNIST model:

- **Fashion-MNIST**:
  - Flattened images passed through a network with 128 and 64 nodes.
  - Output layer has 10 nodes for 10 classes.

- **Titanic**:
  - Deals with strings and numeric columns.
  - Similar network structure with 128 and 64 nodes using the ReLU activation function.
  - Includes a dropout layer to prevent overfitting.
  - Output layer has one node to predict survival.

### Model Compilation

Both models use the Adam optimizer. The Titanic model uses binary cross-entropy for the loss function due to its two classes (survived or not). Both models use accuracy as the metric.

### Training

- **Fashion-MNIST**: Trained using training images and labels for 10 epochs, with 10% of the training data used for validation.
- **Titanic**: Combines columns and labels into a TensorFlow dataset, with validation data from a validation dataset, and runs for 10 epochs.

## TensorFlow 2.0: A Must-Have Skill

TensorFlow 2.0 is becoming one of the most popular deep learning frameworks. It covers foundational skills for deep learning, including creating single and multi-layer networks, training networks, and making predictions.

### Learning Objectives

- Explain the benefits of using TensorFlow.
- Describe the purpose of a rectified linear unit (ReLU).
- Define an epoch concerning training a neural network.
- Interpret how weights are updated when training a neural network.
- Explain the purpose of loss functions.
- Recognize the purpose of an optimizer.
- Define feature engineering.

### Skills Covered

- Neural Networks
- TensorFlow

## `tf.data` API Overview

### Purpose

The `tf.data` API is designed to build efficient input pipelines for training machine learning models. It helps manage and preprocess large datasets, making the training process more efficient.

### Key Methods

- **`from_tensor_slices`**: Creates a dataset from a tensor or a list of tensors. Used to create a dataset from training data and labels.

### Data Preparation

- **DataFrame to Dictionary**: The `from_tensor_slices` method cannot directly accept a DataFrame. Convert the DataFrame into a dictionary using the `dict` function.
- **Shuffling Data**: Crucial during training to ensure the model does not learn patterns based on the order of the data.

### Batching and Prefetching

- **Batching**: Divides the dataset into smaller batches for efficient memory management.
- **Prefetching**: Prepares the next batch of data while the current batch is being processed, ensuring better GPU utilization and reducing training time.

### Practical Example

The video demonstrates creating a function that includes training data, labels, shuffling, batching, and prefetching. The output shows a batch of three passengers from the Titanic dataset, including their ages and survival status.

## Feature Engineering

Feature engineering involves using domain knowledge to extract features from raw data.

### Numeric Columns

- **Normalization**: Applied to numeric columns to ensure machine learning algorithms perform well. Normalization results in a mean of zero and a standard deviation of one for each feature.
- **Function**: `get_normalization_layer` returns a layer that applies feature-wise normalization to numeric features.

### Categorical Columns

- **One-Hot Encoding**: Strings are represented as one-hot encoding vectors using the `get_category_encoding_layer` function.
- **Vocabulary**: Unique values for a column, including an `UNK` token for out-of-vocabulary values.

### Creating the Pipeline

- Define a batch size and prepare train, validation, and test datasets.
- Preprocess numeric and categorical columns to feed them as inputs to the model.
- Store normalized numeric columns and encoded categorical features in lists to pass as inputs to the model.

## Conclusion

This lesson provides a comprehensive overview of working with the Titanic dataset using TensorFlow. It covers data preparation, feature engineering, and the use of the `tf.data` API to build efficient input pipelines.

